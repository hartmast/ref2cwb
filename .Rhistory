The official R software environment is an open-source free software environment within the GNU package, available under the GNU General Public License. It is written primarily in C, Fortran, and R itself (partially self-hosting). Precompiled executables are provided for various operating systems. R has a command line interface. Multiple third-party graphical user interfaces are also available, such as RStudio, an integrated development environment, and Jupyter, a notebook interface. "
tx
gsub("\\[[0-9]\\]", "", tx)
?gsub
gsub("\[[0-9]\]", "", tx)
gsub("\\[[0-9]\\]", "", tx)
"\\[bla"
cat("\\bla")
print("\\bla")
?cat
cat("\\bla")
cat("\\[bla")
library(psy)
library(readxl)
library(tidyverse)
library(readxl)
library(psy)
read_xlsx("/Users/stefanhartmann/sciebo/Projekte/Graphemic variation/Variation_Abiturkorpus/sfb_fehler.xlsx")
d <- read_xlsx("/Users/stefanhartmann/sciebo/Projekte/Graphemic variation/Variation_Abiturkorpus/sfb_fehler.xlsx")
?kappa
d <- read_xlsx("/Users/stefanhartmann/sciebo/Projekte/Graphemic variation/Variation_Abiturkorpus/sfb_fehler.xlsx")
View(d)
View(d)
d <- read_xlsx("/Users/stefanhartmann/sciebo/Projekte/Graphemic variation/Variation_Abiturkorpus/sfb_fehler.xlsx", sheet = "Fehler")
ckappa(cbind(d$`Bsfin Kategorie`, d$`Bsfin Kategorie_SH`))
16+13+16+12+14+15+17+12+8
17+14+14+15+15+5+9+8+6
13+14+12+9+15+8+13+7+5
11+9+16+11+12+13+14+12+3
19+14+12+12+18+13+17+10+4
19+20+20+16+20+24+20+12+8
12+8+12+14+16+13+8+8+6
16+11+16+14+15+10
16+11+16+14+15+10+14+11+8
11+7+16+15+11+13+15+7+8
14+14+12+7.5+8+6+15+7+5
17+10+10+13.5+14+11+16+10+5
19+18+20+16+13+20+15+11+8
19+15+20+13+14+13+18+12+5
18+11+8+14+11+10+18+12+6
19+10+15+12+16+15+13+16
11+5+9+8+14+7+11+12
8+13+11+14+19+17+17+16+20
5+6+5+9+5+14+12+15+10
8+14+12+15+14+14+14+20+12
8+16+6+10+12+10+20+11+8
5+7+8+10+9+12+4
13+9+12+17+9
15+17+10+23+19+8
18+12+10+16+15+8
18+7+8+24+15+8
17+19+10+18+18+8
13+5+13+16+16+4
18+12+12+24+17+8
20+17+11+23+19+8
15+10+13+24+16+8
15+19+15+12+11+22+17+12+8
3500*0.8
3500*0.008
10*1080
10800/60
9+11+7+6+16+15+19+20+16
14+9+11+17+12+13+8+10
19+16+16+16+18+24+20+12+6
12+15+12+20+20+18+20+19+16
8+13+9+20+17+15+4
15+20+20+20+20+18+20+24
13+18+19+20+20+19+18+24
65/3
(65/3)/12
4.3/152
4.3/152*1e6
3.8e6/147
3.8e6/147*1e6
152/4.2*1e6
152/(4.2*1e6)
(152/(4.2*1e6))*1e6
(147/(3.8*1e6))*1e6
3345897*(1/320)
5728-34
2022-2004
52*30
514.37*4
474.80*4
install.packages("polmineR")
install.packages("GermaParl")
use("GermaParl")
library(polmineR)
use("GermaParl")
corpus()
size("GERMAPARL")
size(GERMAPARL)
size("GERMAPARLMINI")
?corpus
devtools::install_github("PolMine/GermaParl", ref = "dev")
75*35
2625/100
100*15
1500/10
95*10
950/100
9.5+26.25+300+100+150
(75*35)/100
(100*10)/100
(100*15)/100
9.5+26.25+30+10+15
(70*35)/100
9.5+24.5+30+10+15
install.packages("RandomForest")
install.packages("randomForest")
library(randomForest)
data(iris)
iris.rf <- randomForest(iris[,-5], iris[,5], prox=TRUE)
iris.p <- classCenter(iris[,-5], iris[,5], iris.rf$prox)
iris.rf
plot(iris.rf)
partialPlot(iris.rf)
str(iris.rf)
varImpPlot(iris.rf)
getTree(randomForest(iris[,-5], iris[,5], ntree=10), 3, labelVar=TRUE)
plot(getTree(randomForest(iris[,-5], iris[,5], ntree=10), 3, labelVar=TRUE))
getTree(randomForest(iris[,-5], iris[,5], ntree=10), 3, labelVar=TRUE)
43.3*318
43.3*(318^0.32)
2519 / (43.3*(318^0.32))
devtools::install_github("hartmast/concordances")
devtools::install_github("hartmast/concordances")
library(concordances)
7.99*12
2 / 70
?install.packages
# subject noun - MHG
d$subject_noun <- character(nrow(d))
0.78*121.51
seq(94, 160, length.out = 11)
0.78*129.85
seq(101, 160, length.out = 11)
seq(80, 160, length.out = 11)
seq(50, 100, length.out = 11)
mean(c(88,73,94,93,87,86))*0.78
seq(80, 160, length.out = 11)
mean(c(17,27,13,13,17,17,17,10,30,23))
devtools::install_github("concordances")
devtools::install_github("hartmastconcordances")
devtools::install_github("hartmast/concordances")
20*14
4*280
1120*12
(14*10)*12
(14*11)*12
910/14
65/4
910*2
910*4
7*52*14
9*52*14
9*26*14
200*0.75
77400/2
7*52*14
7*26*14
3/20
install.packages("word2vec")
install.packages("udpipe")
library(word2vec)
library(udpipe)
data(brussels_reviews_anno, package = "udpipe")
x <- subset(brussels_reviews_anno, language == "fr" & !is.na(lemma) & nchar(lemma) > 1)
x <- subset(x, xpos %in% c("NN", "IN", "RB", "VB", "DT", "JJ", "PRP", "CC",
"VBN", "NNP", "NNS", "PRP$", "CD", "WP", "VBG", "UH", "SYM"))
x$text <- sprintf("%s//%s", x$lemma, x$xpos)
x <- paste.data.frame(x, term = "text", group = "doc_id", collapse = " ")
x
model     <- word2vec(x = x$text, dim = 15, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
citation("word2vec")
library(tidyverse)
library(tidytext)
library(word2vec)
load(url("https://cbail.github.io/Elected_Official_Tweets.Rdata"))
# We want to use original tweets, not retweets:
elected_no_retweets <- elected_official_tweets %>%
filter(is_retweet == F) %>%
select(c("text"))
#create tweet id
elected_no_retweets$postID<-row.names(elected_no_retweets)
install.packages("widyr")
library(widyr)
#create context window with length 8
tidy_skipgrams <- elected_no_retweets %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
#calculate unigram probabilities (used to normalize skipgram probabilities later)
unigram_probs <- elected_no_retweets %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
#calculate probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
#normalize probabilities
normalized_prob <- skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2)
?widyr
normalized_prob[2005:2010,]
install.packages("irlbar")
install.packages("irlba")
library(irlba)
pmi_matrix <- normalized_prob %>%
mutate(pmi = log10(p_together)) %>%
cast_sparse(word1, word2, pmi)
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0
#run SVD
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)
install.packages("keras")
library(reticulate)
library(purrr)
library(text2vec)
install.packages("text2vec")
install.packages("Rtsne")
library(text2vec)
library(Rtsne)
library(plotly)
library(stringr)
library(keras)
# install Keras
install_keras()
library(tidyverse)
library(tidytext)
library(word2vec)
library(widyr)
library(irlba)
library(reticulate)
library(purrr)
library(text2vec)
library(Rtsne)
library(plotly)
library(stringr)
library(keras)
load(url("https://cbail.github.io/Elected_Official_Tweets.Rdata"))
# We want to use original tweets, not retweets:
elected_no_retweets <- elected_official_tweets %>%
filter(is_retweet == F) %>%
select(c("screen_name", "text"))
# Many tweets contain URLs, which we don't want considered in the model:
elected_no_retweets$text <- str_replace_all(string = elected_no_retweets$text,
pattern = "https.+",
replacement = "")
# tokenize text
tokenizer <- text_tokenizer(num_words = 20000)
tokenizer %>% fit_text_tokenizer(elected_no_retweets$text)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
# Number of Dimensions in the embedding vector.
embedding_size <- 128
# Size of context window
skip_window <- 5
# Number of negative examples to sample for each word.
num_sampled <- 1
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
embedding <- layer_embedding(
input_dim = tokenizer$num_words + 1,
output_dim = embedding_size,
input_length = 1,
name = "embedding"
)
target_vector <- input_target %>%
embedding() %>%
layer_flatten()
context_vector <- input_context %>%
embedding() %>%
layer_flatten()
dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")
model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
summary(model)
model %>%
fit_generator(
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 2
)
model %>%
fit_generator(
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10
)
?fit_generator
fit_generator()
?fit()
model %>%
fit(
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10
)
?`keras-package`
version("keras")
str(model)
model
fit(model, skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10
)
elected_no_retweets
fit(model, skipgrams_generator(as.data.frame(elected_no_retweets$text), tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10)
fit(model, skipgrams_generator(as.data.frame(elected_no_retweets$text), tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10)
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples)
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples)
?generator_next
?texts_to_sequences_generator
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
rm(list = ls())
library(tidyverse)
library(tidytext)
52*70
5950-350
40*0.65
46605/2
23303*2
46605*2
50*300
30*300
190*200
170*200
38700/2
667 + 725
36*300
mean(c(35,65))
50*300
1800+1150+300
275250*(0.25)
68812.5+226341+51926
68812.5+226341+51926+12000
32+26+5+2
6293.21*3
(93210*2)
(93210*4)
124005*4
496020/2
data("mtcars")
mtcars
7000*0.04
47+1
47+14
600/5
230-150
73+65
(184*2)+(195*10)
2318*(12*80)
2318+(12*80)
2880/12
330*12
240*12
235+31+11+45+3
396+35
2880+1205
8364/12
230-120
2318/12
9120.74+51168.30
345/4
setwd("~/sciebo/Corpora/ReF-v1.0.2")
j = 1
library(tidyverse)
# list files
f_mlu <- list.files("ref-mlu/", full.names = T)
f_rub <- list.files("ref-rub/", full.names = T)
f <- c(f_mlu, f_rub)
# read data
d <- readLines(f[j])
# get metadata
header_name <- gsub("\".*", "", gsub(".*name=\"", "", grep("<cora-header .* name", d, value = T)))
# get header
header_start <- grep("<header", d)
header_end   <- grep("</header", d)
hdr <- d[header_start:header_end]
# remove all quotation marks to avoid syntax problems
hdr <- gsub("\"", "", hdr)
# get header infos
hdr <- sub(":", "=\"", hdr)
hdr <- gsub("(?<=$)", "\"", hdr, perl = T)
text_header <- paste0("<text id=\"", header_name, "\"", gsub("text=\"", "text_name=\"", gsub("</?header>", "", paste0(hdr, collapse = " "))), ">", collapse = " ")
# get tokens
token_start <- grep("<tok_anno", d)
token_end   <- grep("</tok_anno", d)
i = 1
cur <-d[token_start[i]:token_end[i]]
cur
i = 10
cur <-d[token_start[i]:token_end[i]]
cur
j = 1
# read data
d <- readLines(f[j])
# get metadata
header_name <- gsub("\".*", "", gsub(".*name=\"", "", grep("<cora-header .* name", d, value = T)))
# get header
header_start <- grep("<header", d)
header_end   <- grep("</header", d)
hdr <- d[header_start:header_end]
# remove all quotation marks to avoid syntax problems
hdr <- gsub("\"", "", hdr)
# get header infos
hdr <- sub(":", "=\"", hdr)
hdr <- gsub("(?<=$)", "\"", hdr, perl = T)
text_header <- paste0("<text id=\"", header_name, "\"", gsub("text=\"", "text_name=\"", gsub("</?header>", "", paste0(hdr, collapse = " "))), ">", collapse = " ")
# get tokens
token_start <- grep("<tok_anno", d)
token_end   <- grep("</tok_anno", d)
# empty table for dipl, lemma, POS, morph
cur_tbl <- tibble(
word      = character(length(token_start)),
token_id  = character(length(token_start)),
tok_anno      = character(length(token_start)),
lemma     = character(length(token_start)),
pos       = character(length(token_start)),
pos_lemma = character(length(token_start)),
morph     = character(length(token_start)),
lemma_id     = character(length(token_start)),
anno_type =  character(length(token_start)),
punc     = character(length(token_start))
)
# fill table
for(i in 1:length(token_start)) {
cur <-d[token_start[i]:token_end[i]]
if(length(grep(".*<tok_anno id=\"", cur)) > 0) {
cur_tbl$token_id[i] <- gsub("\".*", "", gsub(".*<tok_anno id=\"", "", cur[grep("<tok_anno", cur)]))
} else {
cur_tbl$token_id[i] <- "-"
}
if(length(grep(".*<tok_anno.*utf", cur)) > 0) {
cur_tbl$tok_anno[i] <- gsub("\".*", "", gsub(".*utf=\"", "", cur[grep("<tok_anno", cur)]))
} else {
cur_tbl$tok_anno[i] <- "-"
}
if(length(grep(".*<tok_anno.*ascii", cur)) > 0) {
cur_tbl$word[i] <- gsub("\".*", "", gsub(".*ascii=\"", "", cur[grep("<tok_anno", cur)]))
} else {
cur_tbl$word[i] <- "-"
}
if(length(grep(".*<lemma tag=\"", cur)) > 0) {
cur_tbl$lemma[i] <- gsub("\".*", "", gsub(".*<lemma tag=\"", "", cur[grep("lemma tag", cur)]))
} else {
cur_tbl$lemma[i] <- "-"
}
if(length(grep(".*<posLemma tag=\"", cur)) > 0) {
cur_tbl$pos_lemma[i] <- gsub("\".*", "", gsub(".*<posLemma tag=\"", "", cur[grep("posLemma tag", cur)]))
} else {
cur_tbl$pos_lemma[i] <- "-"
}
if(length(grep(".*<morph tag=\"", cur)) > 0) {
cur_tbl$morph[i] <- gsub("\".*", "", gsub(".*<morph tag=\"", "", cur[grep("morph tag", cur)]))
} else {
cur_tbl$morph[i] <- "-"
}
if(length(grep(".*<pos tag=\"", cur)) > 0) {
cur_tbl$pos[i] <- gsub("\".*", "", gsub(".*<pos tag=\"", "", cur[grep("pos tag", cur)]))
} else {
cur_tbl$pos[i] <- "-"
}
if(length(grep(".*<punc tag=\"", cur)) > 0) {
cur_tbl$punc[i] <- gsub("\".*", "", gsub(".*<punc tag=\"", "", cur[grep("punc tag", cur)]))
} else {
cur_tbl$punc[i] <- "-"
}
if(length(grep(".*<annoType tag=\"", cur)) > 0) {
cur_tbl$anno_type[i] <- gsub("\".*", "", gsub(".*<annoType tag=\"", "", cur[grep("annoType tag", cur)]))
} else {
cur_tbl$anno_type[i] <- "-"
}
#print(i)
}
View(cur_tbl)
